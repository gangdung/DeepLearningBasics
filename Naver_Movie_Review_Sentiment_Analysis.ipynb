{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naver_Movie_Review_Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2mAuZxQsylz6DE+DB3mSM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangdung/DeepLearningBasics/blob/master/Naver_Movie_Review_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr6PTelD70X4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGommZTk71UK"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplot as mp\r\n",
        "import matplotlib\r\n",
        "#matplotlib.use('agg')  #matplotlib는 GUI 이벤트 루프에 연결되어 예기치 않은 동작을 일으키는 \"tkagg\"백엔드에 액세스, 일반 \"agg\"백엔드는 GUI에 전혀 연결되지 않습니다.\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import urllib.request\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\r\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\r\n",
        "\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "train_data = pd.read_table('ratings_train.txt')\r\n",
        "test_data = pd.read_table('ratings_test.txt')\r\n",
        "\r\n",
        "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\r\n",
        "print('상위 5개 출력(train_data) : ', train_data[:5])  # 상위 5개 출력\r\n",
        "print('상위 5개 출력(test_data)  : ', test_data[:5])  # 상위 5개 출력\r\n",
        "\r\n",
        "# 데이타검증\r\n",
        "print(train_data['document'].nunique(), train_data['label'].nunique()) #document열에서 중복을 제거한 샘플의 개수\r\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\r\n",
        "print('총 샘플의 수(null제거전):',len(train_data))\r\n",
        "\r\n",
        "# 박스플롯 띄우기\r\n",
        "#train_data['label'].value_counts().plot(kind = 'bar')\r\n",
        "#plt.show()\r\n",
        "\r\n",
        "print('label건수비교 : ' , train_data.groupby('label').size().reset_index(name = 'count'))\r\n",
        "print('Null값Check : ' , train_data.isnull().values.any())\r\n",
        "\r\n",
        "print(train_data.isnull().sum())\r\n",
        "print(train_data.loc[train_data.document.isnull()])\r\n",
        "\r\n",
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\r\n",
        "print('Null값Check : ' , train_data.isnull().values.any()) # Null 값이 존재하는지 확인\r\n",
        "\r\n",
        "print('총 샘플의 수(null제거후):',len(train_data))\r\n",
        "\r\n",
        "#데이터 전처리\r\n",
        "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\r\n",
        "print(re.sub(r'[^a-zA-Z ]', '', text)) #알파벳과 공백을 제외하고 모두 제거\r\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글과 공백을 제외하고 모두 제거\r\n",
        "print(train_data[:5])\r\n",
        "\r\n",
        "train_data['document'].replace('', np.nan, inplace=True)\r\n",
        "print(train_data.isnull().sum())\r\n",
        "print(train_data.loc[train_data.document.isnull()][:5])\r\n",
        "\r\n",
        "train_data = train_data.dropna(how = 'any')\r\n",
        "print('총 샘플의 수(null+한글외문자제거후):',len(train_data))\r\n",
        "\r\n",
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\r\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\r\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\r\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\r\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\r\n",
        "okt = Okt()\r\n",
        "print('Test:', okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True))\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\r\n",
        "X_train = []\r\n",
        "for sentence in train_data['document']:\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
        "    X_train.append(temp_X)\r\n",
        "\r\n",
        "print('상위 3개의 샘플만 출력(X_train) : ', X_train[:3])\r\n",
        "\r\n",
        "X_test = []\r\n",
        "for sentence in test_data['document']:\r\n",
        "    temp_X = []\r\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\r\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
        "    X_test.append(temp_X)\r\n",
        "\r\n",
        "print('상위 3개의 샘플만 출력(X_test)  : ', X_test[:3])\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "print('각 단어에 고유한 정수가 부여 : ', tokenizer.word_index)\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "threshold = 3\r\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\r\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\r\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n",
        "\r\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n",
        "for key, value in tokenizer.word_counts.items():\r\n",
        "    total_freq = total_freq + value\r\n",
        "\r\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\r\n",
        "    if(value < threshold):\r\n",
        "        rare_cnt = rare_cnt + 1\r\n",
        "        rare_freq = rare_freq + value\r\n",
        "\r\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\r\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\r\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\r\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\r\n",
        "\r\n",
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\r\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\r\n",
        "vocab_size = total_cnt - rare_cnt + 2\r\n",
        "print('단어 집합의 크기 :',vocab_size)\r\n",
        "\r\n",
        "# 정수 인코딩 과정\r\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "print('정수 인코딩 :', X_train[:3])\r\n",
        "\r\n",
        "y_train = np.array(train_data['label'])\r\n",
        "y_test = np.array(test_data['label'])\r\n",
        "\r\n",
        "#####5.빈 샘플(empty samples) 제거\r\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\r\n",
        "\r\n",
        "# 빈 샘플들을 제거\r\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\r\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "#####6.패딩\r\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\r\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\r\n",
        "#plt.hist([len(s) for s in X_train], bins=50)\r\n",
        "#plt.xlabel('length of samples')\r\n",
        "#plt.ylabel('number of samples')\r\n",
        "#plt.show()\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "def below_threshold_len(max_len, nested_list):\r\n",
        "  cnt = 0\r\n",
        "  for s in nested_list:\r\n",
        "    if(len(s) <= max_len):\r\n",
        "        cnt = cnt + 1\r\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\r\n",
        "\r\n",
        "max_len = 30\r\n",
        "below_threshold_len(max_len, X_train)\r\n",
        "\r\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\r\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)\r\n",
        "\r\n",
        "print('상위 3개의 샘플만 출력(X_train) : ', X_train[:3])\r\n",
        "print('상위 3개의 샘플만 출력(X_test)  : ', X_test[:3])\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 100))\r\n",
        "model.add(LSTM(128))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\r\n",
        "\r\n",
        "loaded_model_naver = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_naver.evaluate(X_test, y_test)[1]))\r\n",
        "\r\n",
        "def sentiment_predict(old_sentence):\r\n",
        "  new_sentence = okt.morphs(old_sentence, stem=True) # 토큰화\r\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\r\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\r\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n",
        "  score = float(loaded_model_naver.predict(pad_new)) # 예측\r\n",
        "  if(score > 0.5):\r\n",
        "    print(old_sentence, ' : ', \"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\r\n",
        "  else:\r\n",
        "    print(old_sentence, ' : ', \"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\r\n",
        "\r\n",
        "\r\n",
        "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')\r\n",
        "sentiment_predict('이 영화 핵노잼 ㅠㅠ')\r\n",
        "sentiment_predict('이딴게 영화냐 ㅉㅉ')\r\n",
        "sentiment_predict('감독 뭐하는 놈이냐?')\r\n",
        "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')\r\n",
        "\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "print('-------------------------------------------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}