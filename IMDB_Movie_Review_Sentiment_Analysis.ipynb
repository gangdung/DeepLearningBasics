{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Movie_Review_Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOW1QIGcgDvut2schtxe58x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangdung/DeepLearningBasics/blob/master/IMDB_Movie_Review_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HwTFqF6Ai0G"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplot as mp\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.datasets import imdb\r\n",
        "\r\n",
        "# 영화 리뷰는 X_train에, 감성 정보는 y_train에 저장된다.\r\n",
        "# 테스트용 리뷰는 X_test에, 테스트용 리뷰의 감성 정보는 y_test에 저장된다.\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\r\n",
        "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = 10000)\r\n",
        "\r\n",
        "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\r\n",
        "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))\r\n",
        "num_classes = max(y_train) + 1\r\n",
        "print('카테고리 : {}'.format(num_classes))\r\n",
        "\r\n",
        "print(X_train[0])\r\n",
        "print(y_train[0])\r\n",
        "\r\n",
        "len_result = [len(s) for s in X_train]\r\n",
        "\r\n",
        "print('리뷰의 최대 길이 : {}'.format(np.max(len_result)))\r\n",
        "print('리뷰의 평균 길이 : {}'.format(np.mean(len_result)))\r\n",
        "\r\n",
        "plt.subplot(1,2,1)\r\n",
        "plt.boxplot(len_result)\r\n",
        "plt.subplot(1,2,2)\r\n",
        "plt.hist(len_result, bins=50)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\r\n",
        "print(\"각 레이블에 대한 빈도수:\")\r\n",
        "print(np.asarray((unique_elements, counts_elements)))\r\n",
        "\r\n",
        "\r\n",
        "word_to_index = imdb.get_word_index()\r\n",
        "index_to_word={}\r\n",
        "for key, value in word_to_index.items():\r\n",
        "    index_to_word[value+3] = key\r\n",
        "\r\n",
        "print('빈도수 상위 1등 단어 : {}'.format(index_to_word[4]))\r\n",
        "print('빈도수 상위 3938등 단어 : {}'.format(index_to_word[3941]))\r\n",
        "\r\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\r\n",
        "  index_to_word[index]=token\r\n",
        "\r\n",
        "print(' '.join([index_to_word[index] for index in X_train[0]]))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "import re\r\n",
        "from tensorflow.keras.datasets import imdb\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "\r\n",
        "vocab_size = 10000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\r\n",
        "\r\n",
        "max_len = 500\r\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\r\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 100))\r\n",
        "model.add(GRU(128))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\r\n",
        "\r\n",
        "loaded_model_imdb = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_imdb.evaluate(X_test, y_test)[1]))\r\n",
        "\r\n",
        "\r\n",
        "def sentiment_predict(old_sentence):\r\n",
        "  # 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\r\n",
        "  new_sentence = re.sub('[^0-9a-zA-Z ]', '', old_sentence).lower()\r\n",
        "\r\n",
        "  # 정수 인코딩\r\n",
        "  encoded = []\r\n",
        "  for word in new_sentence.split():\r\n",
        "    # 단어 집합의 크기를 10,000으로 제한.\r\n",
        "    try :\r\n",
        "      if word_to_index[word] <= 10000:\r\n",
        "        encoded.append(word_to_index[word]+3)\r\n",
        "      else:\r\n",
        "    # 10,000 이상의 숫자는 <unk> 토큰으로 취급.\r\n",
        "        encoded.append(2)\r\n",
        "    # 단어 집합에 없는 단어는 <unk> 토큰으로 취급.\r\n",
        "    except KeyError:\r\n",
        "      encoded.append(2)\r\n",
        "\r\n",
        "  pad_new = pad_sequences([encoded], maxlen = max_len) # 패딩\r\n",
        "  score = float(loaded_model_imdb.predict(pad_new)) # 예측\r\n",
        "  if(score > 0.5):\r\n",
        "    print(old_sentence, '\\n : ',\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\r\n",
        "  else:\r\n",
        "    print(old_sentence, '\\n : ',\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\r\n",
        "\r\n",
        "temp_str = \"This movie was just way too overrated. The fighting was not professional and in slow motion. I was expecting more from a 200 million budget movie. The little sister of T.Challa was just trying too hard to be funny. The story was really dumb as well. Don't watch this movie if you are going because others say its great unless you are a Black Panther fan or Marvels fan.\"\r\n",
        "sentiment_predict(temp_str)\r\n",
        "\r\n",
        "temp_str = \" I was lucky enough to be included in the group to see the advanced screening in Melbourne on the 15th of April, 2012. And, firstly, I need to say a big thank-you to Disney and Marvel Studios. \\\r\n",
        "Now, the film... how can I even begin to explain how I feel about this film? It is, as the title of this review says a 'comic book triumph'. I went into the film with very, very high expectations and I was not disappointed. \\\r\n",
        "Seeing Joss Whedon's direction and envisioning of the film come to life on the big screen is perfect. The script is amazingly detailed and laced with sharp wit a humor. The special effects are literally mind-blowing and the action scenes are both hard-hitting and beautifully choreographed.\"\r\n",
        "sentiment_predict(temp_str)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}